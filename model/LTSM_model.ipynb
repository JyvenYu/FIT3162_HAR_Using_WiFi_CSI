{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human Activity Recognition (HAR) Experiment Using LSTM\n",
    "1.\tData Collection\n",
    "\n",
    "        • Combine data from different data collection sessions to form dataset.\n",
    "\n",
    "2.\tData Preprocessing\n",
    "\n",
    "        • Standardize Light Data & CSI Data:\n",
    "\n",
    "        • Apply standardization to the Channel State Information (CSI) data. This involves scaling the data so that it has a mean of zero and a standard deviation of one. \n",
    "\n",
    "3.\tTrain-Test Split\n",
    "\n",
    "        • 80:20 Split:\n",
    "\n",
    "        • Divide the dataset into training and testing sets with an 80:20 ratio. This means 80% of the data will be used for training the model, and 20% will be reserved for testing its performance.\n",
    "\n",
    "4.\tModel Architecture Definition\n",
    "\n",
    "        • Design LSTM Network:\n",
    "\n",
    "        • Define the LSTM model architecture suitable for HAR. \n",
    "\n",
    "5.\tModel Compilation\n",
    "\n",
    "        • Compile the LSTM Model:\n",
    "\n",
    "        • Compile the model with an appropriate optimizer (like Adam), loss function (such as categorical_crossentropy for multi-class classification), and metrics (like accuracy).\n",
    "\n",
    "6.\tModel Training\n",
    "\n",
    "        • Training with Validation Split:\n",
    "\n",
    "        • Train the model on the training dataset while using a validation split of 0.2. This means that 20% of the training data is used as a validation set to monitor the model’s performance and help in tuning the hyperparameters.\n",
    "\n",
    "7.\tModel Evaluation\n",
    "\n",
    "        • Evaluate Model on Test Set:\n",
    "\n",
    "        • After training, evaluate the model's performance on the test set. \n",
    "\n",
    "8.\tPerformance Metrics and Visualization\n",
    "\n",
    "        • Analyze Results:\n",
    "\n",
    "        • Use various performance metrics like accuracy, confusion matrix, precision, recall, F1-score, etc., to evaluate the model. \n",
    "\n",
    "9.\tModel Saving \n",
    "\n",
    "        • Save the Trained Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "data_directory = r\"..\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for data processing\n",
    "PILOT_SUBCARRIERS = ['csi_channel_' + str(i+32) for i in [-21, -7, 21,  7]]\n",
    "NULL_SUBCARRIERS = ['csi_channel_' + str(i+32) for i in [-32, -31, -30, -29, 31,  30,  29,  0]]\n",
    "SUBCARRIERS_TO_REMOVE = PILOT_SUBCARRIERS + NULL_SUBCARRIERS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load CSV data\n",
    "def load_csv_data(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            label = filename.split(\"_\")[0]\n",
    "            yield filepath, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from directory\n",
    "activity_files = list(load_csv_data(data_directory))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for data processing\n",
    "def process_data(file_paths):\n",
    "    data = []\n",
    "    for path in file_paths:\n",
    "        df = pd.read_csv(path, header=None)\n",
    "        data.append(df.values)\n",
    "    return data\n",
    "\n",
    "def assign_labels(data, label):\n",
    "    labels = [label for _ in range(len(data))]\n",
    "    return np.array(labels).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "categories = ['empty', 'stand', 'sit', 'walk']\n",
    "file_paths = {cat: [f for f, lbl in activity_files if lbl == cat] for cat in categories}\n",
    "\n",
    "data = {cat: process_data(file_paths[cat]) for cat in categories}\n",
    "labels = {cat: assign_labels(data[cat], cat) for cat in categories}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data concatenation and shuffling\n",
    "X_all = np.vstack([data[cat] for cat in categories])\n",
    "y_all = np.vstack([labels[cat] for cat in categories])\n",
    "X_all, y_all = shuffle(X_all, y_all, random_state=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class DataScaler(StandardScaler):\n",
    "    def __init__(self, **kwargs):\n",
    "        self._scaler = StandardScaler(copy=True, **kwargs)\n",
    "        self._orig_shape = None\n",
    "\n",
    "    def fit(self, X, **kwargs):\n",
    "        X = np.array(X)\n",
    "        # Save the original shape to reshape the flattened X later\n",
    "        # back to its original shape\n",
    "        if len(X.shape) > 1:\n",
    "            self._orig_shape = X.shape[1:]\n",
    "        X = self._flatten(X)\n",
    "        self._scaler.fit(X, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **kwargs):\n",
    "        X = np.array(X)\n",
    "        X = self._flatten(X)\n",
    "        X = self._scaler.transform(X, **kwargs)\n",
    "        X = self._reshape(X)\n",
    "        return X\n",
    "\n",
    "    def _flatten(self, X):\n",
    "        # Reshape X to <= 2 dimensions\n",
    "        if len(X.shape) > 2:\n",
    "            n_dims = np.prod(self._orig_shape)\n",
    "            X = X.reshape(-1, n_dims)\n",
    "        return X\n",
    "\n",
    "    def _reshape(self, X):\n",
    "        # Reshape X back to it's original shape\n",
    "        if len(X.shape) >= 2:\n",
    "            X = X.reshape(-1, *self._orig_shape)\n",
    "        return X\n",
    "\n",
    "# Apply scaling\n",
    "scaler = DataScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train).toarray()\n",
    "y_test_encoded = encoder.transform(y_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "\n",
    "# Save scaler and encoder\n",
    "os.makedirs('v6_obj', exist_ok=True)\n",
    "dump(scaler, open('v6_obj/scaler.pkl', 'wb'))\n",
    "dump(encoder, open('v6_obj/encoder.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining LSTM Architecture\n",
    "lstm_net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(48, input_shape=X_train[-2,:].shape),  # Adjusted input_shape\n",
    "    tf.keras.layers.Dropout(0.1),  # Dropout layer\n",
    "    tf.keras.layers.Dense(16, activation='relu'), \n",
    "    tf.keras.layers.Dense(y_train_encoded.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "def lr_scheduler(epoch, lr):\n",
    "    return 1e-8 * 10**(epoch / 20)\n",
    "\n",
    "schedule = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Optimizer Configuration\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-8)\n",
    "lstm_net.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Training the Model\n",
    "history = lstm_net.fit(X_train, y_train_encoded, epochs=150, validation_split=0.2, batch_size=16, callbacks=[schedule])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualizing Learning Rate Impact\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Learning Rate vs Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback for Early Stopping\n",
    "class TrainingCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if logs.get('val_accuracy') > 0.98:\n",
    "            print(\"\\nReached 98% accuracy. Stopping training.\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "my_callback = TrainingCallback()\n",
    "\n",
    "# LSTM Network Reinitialization for Training with Optimal Learning Rate\n",
    "final_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(48, input_shape=X_train[-2,:].shape),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(16, activation='relu'), \n",
    "    tf.keras.layers.Dense(y_train_encoded.shape[1], activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Compilation and Training\n",
    "final_model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), metrics=['accuracy'])\n",
    "final_history = final_model.fit(X_train, y_train_encoded, epochs=500, validation_split=0.2, batch_size=16, callbacks=[my_callback, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation on Test Data\n",
    "final_model.evaluate(X_test, y_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Plotting learning rate vs loss\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
    "sns.set(font_scale=1.5)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.title('Learning Rate vs Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Training History\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(final_history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(final_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# Confusion Matrix and Classification Report\n",
    "y_pred = final_model.predict(X_test)\n",
    "conf_matrix = confusion_matrix(np.argmax(y_test_encoded, axis=1), np.argmax(y_pred, axis=1))\n",
    "activity_labels = ['Empty', 'Sit', 'Stand', 'Walk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_df = pd.DataFrame(conf_matrix, index=activity_labels, columns=activity_labels)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_df, annot=True, fmt='g', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Displaying Classification Report\n",
    "print(classification_report(np.argmax(y_test_encoded, axis=1), np.argmax(y_pred, axis=1), target_names=activity_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Final Model\n",
    "final_model.save('final_activity_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
